{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c4ca3c6-259e-4f51-8570-1f98e72e1d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "DataLoader num_workers set to: 0\n",
      "Loading train_dataset...\n",
      "train_dataset loaded.\n",
      "Loading val_dataset...\n",
      "val_dataset loaded.\n",
      "Creating DataLoaders...\n",
      "DataLoaders created.\n",
      "Dataset sizes: {'train': 50000, 'val': 10000}\n",
      "Class names: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "Creating model...\n",
      "Model created and modified.\n",
      "\n",
      "Epoch 0/9\n",
      "----------\n",
      "Starting train phase for Epoch 0...\n",
      "   Train Batch [1/1563]\n",
      "   Train Batch [100/1563]\n",
      "   Train Batch [200/1563]\n",
      "   Train Batch [300/1563]\n",
      "   Train Batch [400/1563]\n",
      "   Train Batch [500/1563]\n",
      "   Train Batch [600/1563]\n",
      "   Train Batch [700/1563]\n",
      "   Train Batch [800/1563]\n",
      "   Train Batch [900/1563]\n",
      "   Train Batch [1000/1563]\n",
      "   Train Batch [1100/1563]\n",
      "   Train Batch [1200/1563]\n",
      "   Train Batch [1300/1563]\n",
      "   Train Batch [1400/1563]\n",
      "   Train Batch [1500/1563]\n",
      "   Train Batch [1563/1563]\n",
      "Finished train phase for Epoch 0.\n",
      "train Loss: 0.9862 Acc: 0.6561\n",
      "Starting val phase for Epoch 0...\n",
      "   Val Batch [1/313]\n",
      "   Val Batch [100/313]\n",
      "   Val Batch [200/313]\n",
      "   Val Batch [300/313]\n",
      "   Val Batch [313/313]\n",
      "Finished val phase for Epoch 0.\n",
      "val Loss: 0.3627 Acc: 0.8778\n",
      "\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "Starting train phase for Epoch 1...\n",
      "   Train Batch [1/1563]\n",
      "   Train Batch [100/1563]\n",
      "   Train Batch [200/1563]\n",
      "   Train Batch [300/1563]\n",
      "   Train Batch [400/1563]\n",
      "   Train Batch [500/1563]\n",
      "   Train Batch [600/1563]\n",
      "   Train Batch [700/1563]\n",
      "   Train Batch [800/1563]\n",
      "   Train Batch [900/1563]\n",
      "   Train Batch [1000/1563]\n",
      "   Train Batch [1100/1563]\n",
      "   Train Batch [1200/1563]\n",
      "   Train Batch [1300/1563]\n",
      "   Train Batch [1400/1563]\n",
      "   Train Batch [1500/1563]\n",
      "   Train Batch [1563/1563]\n",
      "Finished train phase for Epoch 1.\n",
      "train Loss: 0.7076 Acc: 0.7538\n",
      "Starting val phase for Epoch 1...\n",
      "   Val Batch [1/313]\n",
      "   Val Batch [100/313]\n",
      "   Val Batch [200/313]\n",
      "   Val Batch [300/313]\n",
      "   Val Batch [313/313]\n",
      "Finished val phase for Epoch 1.\n",
      "val Loss: 0.2971 Acc: 0.8993\n",
      "\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "Starting train phase for Epoch 2...\n",
      "   Train Batch [1/1563]\n",
      "   Train Batch [100/1563]\n",
      "   Train Batch [200/1563]\n",
      "   Train Batch [300/1563]\n",
      "   Train Batch [400/1563]\n",
      "   Train Batch [500/1563]\n",
      "   Train Batch [600/1563]\n",
      "   Train Batch [700/1563]\n",
      "   Train Batch [800/1563]\n",
      "   Train Batch [900/1563]\n",
      "   Train Batch [1000/1563]\n",
      "   Train Batch [1100/1563]\n",
      "   Train Batch [1200/1563]\n",
      "   Train Batch [1300/1563]\n",
      "   Train Batch [1400/1563]\n",
      "   Train Batch [1500/1563]\n",
      "   Train Batch [1563/1563]\n",
      "Finished train phase for Epoch 2.\n",
      "train Loss: 0.6382 Acc: 0.7790\n",
      "Starting val phase for Epoch 2...\n",
      "   Val Batch [1/313]\n",
      "   Val Batch [100/313]\n",
      "   Val Batch [200/313]\n",
      "   Val Batch [300/313]\n",
      "   Val Batch [313/313]\n",
      "Finished val phase for Epoch 2.\n",
      "val Loss: 0.3745 Acc: 0.8781\n",
      "\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "Starting train phase for Epoch 3...\n",
      "   Train Batch [1/1563]\n",
      "   Train Batch [100/1563]\n",
      "   Train Batch [200/1563]\n",
      "   Train Batch [300/1563]\n",
      "   Train Batch [400/1563]\n",
      "   Train Batch [500/1563]\n",
      "   Train Batch [600/1563]\n",
      "   Train Batch [700/1563]\n",
      "   Train Batch [800/1563]\n",
      "   Train Batch [900/1563]\n",
      "   Train Batch [1000/1563]\n",
      "   Train Batch [1100/1563]\n",
      "   Train Batch [1200/1563]\n",
      "   Train Batch [1300/1563]\n",
      "   Train Batch [1400/1563]\n",
      "   Train Batch [1500/1563]\n",
      "   Train Batch [1563/1563]\n",
      "Finished train phase for Epoch 3.\n",
      "train Loss: 0.5871 Acc: 0.7975\n",
      "Starting val phase for Epoch 3...\n",
      "   Val Batch [1/313]\n",
      "   Val Batch [100/313]\n",
      "   Val Batch [200/313]\n",
      "   Val Batch [300/313]\n",
      "   Val Batch [313/313]\n",
      "Finished val phase for Epoch 3.\n",
      "val Loss: 0.2232 Acc: 0.9234\n",
      "\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "Starting train phase for Epoch 4...\n",
      "   Train Batch [1/1563]\n",
      "   Train Batch [100/1563]\n",
      "   Train Batch [200/1563]\n",
      "   Train Batch [300/1563]\n",
      "   Train Batch [400/1563]\n",
      "   Train Batch [500/1563]\n",
      "   Train Batch [600/1563]\n",
      "   Train Batch [700/1563]\n",
      "   Train Batch [800/1563]\n",
      "   Train Batch [900/1563]\n",
      "   Train Batch [1000/1563]\n",
      "   Train Batch [1100/1563]\n",
      "   Train Batch [1200/1563]\n",
      "   Train Batch [1300/1563]\n",
      "   Train Batch [1400/1563]\n",
      "   Train Batch [1500/1563]\n",
      "   Train Batch [1563/1563]\n",
      "Finished train phase for Epoch 4.\n",
      "train Loss: 0.5553 Acc: 0.8084\n",
      "Starting val phase for Epoch 4...\n",
      "   Val Batch [1/313]\n",
      "   Val Batch [100/313]\n",
      "   Val Batch [200/313]\n",
      "   Val Batch [300/313]\n",
      "   Val Batch [313/313]\n",
      "Finished val phase for Epoch 4.\n",
      "val Loss: 0.2387 Acc: 0.9199\n",
      "\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "Starting train phase for Epoch 5...\n",
      "   Train Batch [1/1563]\n",
      "   Train Batch [100/1563]\n",
      "   Train Batch [200/1563]\n",
      "   Train Batch [300/1563]\n",
      "   Train Batch [400/1563]\n",
      "   Train Batch [500/1563]\n",
      "   Train Batch [600/1563]\n",
      "   Train Batch [700/1563]\n",
      "   Train Batch [800/1563]\n",
      "   Train Batch [900/1563]\n",
      "   Train Batch [1000/1563]\n",
      "   Train Batch [1100/1563]\n",
      "   Train Batch [1200/1563]\n",
      "   Train Batch [1300/1563]\n",
      "   Train Batch [1400/1563]\n",
      "   Train Batch [1500/1563]\n",
      "   Train Batch [1563/1563]\n",
      "Finished train phase for Epoch 5.\n",
      "train Loss: 0.5276 Acc: 0.8162\n",
      "Starting val phase for Epoch 5...\n",
      "   Val Batch [1/313]\n",
      "   Val Batch [100/313]\n",
      "   Val Batch [200/313]\n",
      "   Val Batch [300/313]\n",
      "   Val Batch [313/313]\n",
      "Finished val phase for Epoch 5.\n",
      "val Loss: 0.2213 Acc: 0.9260\n",
      "\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "Starting train phase for Epoch 6...\n",
      "   Train Batch [1/1563]\n",
      "   Train Batch [100/1563]\n",
      "   Train Batch [200/1563]\n",
      "   Train Batch [300/1563]\n",
      "   Train Batch [400/1563]\n",
      "   Train Batch [500/1563]\n",
      "   Train Batch [600/1563]\n",
      "   Train Batch [700/1563]\n",
      "   Train Batch [800/1563]\n",
      "   Train Batch [900/1563]\n",
      "   Train Batch [1000/1563]\n",
      "   Train Batch [1100/1563]\n",
      "   Train Batch [1200/1563]\n",
      "   Train Batch [1300/1563]\n",
      "   Train Batch [1400/1563]\n",
      "   Train Batch [1500/1563]\n",
      "   Train Batch [1563/1563]\n",
      "Finished train phase for Epoch 6.\n",
      "train Loss: 0.5052 Acc: 0.8248\n",
      "Starting val phase for Epoch 6...\n",
      "   Val Batch [1/313]\n",
      "   Val Batch [100/313]\n",
      "   Val Batch [200/313]\n",
      "   Val Batch [300/313]\n",
      "   Val Batch [313/313]\n",
      "Finished val phase for Epoch 6.\n",
      "val Loss: 0.2138 Acc: 0.9305\n",
      "\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "Starting train phase for Epoch 7...\n",
      "   Train Batch [1/1563]\n",
      "   Train Batch [100/1563]\n",
      "   Train Batch [200/1563]\n",
      "   Train Batch [300/1563]\n",
      "   Train Batch [400/1563]\n",
      "   Train Batch [500/1563]\n",
      "   Train Batch [600/1563]\n",
      "   Train Batch [700/1563]\n",
      "   Train Batch [800/1563]\n",
      "   Train Batch [900/1563]\n",
      "   Train Batch [1000/1563]\n",
      "   Train Batch [1100/1563]\n",
      "   Train Batch [1200/1563]\n",
      "   Train Batch [1300/1563]\n",
      "   Train Batch [1400/1563]\n",
      "   Train Batch [1500/1563]\n",
      "   Train Batch [1563/1563]\n",
      "Finished train phase for Epoch 7.\n",
      "train Loss: 0.4113 Acc: 0.8591\n",
      "Starting val phase for Epoch 7...\n",
      "   Val Batch [1/313]\n",
      "   Val Batch [100/313]\n",
      "   Val Batch [200/313]\n",
      "   Val Batch [300/313]\n",
      "   Val Batch [313/313]\n",
      "Finished val phase for Epoch 7.\n",
      "val Loss: 0.1498 Acc: 0.9482\n",
      "\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "Starting train phase for Epoch 8...\n",
      "   Train Batch [1/1563]\n",
      "   Train Batch [100/1563]\n",
      "   Train Batch [200/1563]\n",
      "   Train Batch [300/1563]\n",
      "   Train Batch [400/1563]\n",
      "   Train Batch [500/1563]\n",
      "   Train Batch [600/1563]\n",
      "   Train Batch [700/1563]\n",
      "   Train Batch [800/1563]\n",
      "   Train Batch [900/1563]\n",
      "   Train Batch [1000/1563]\n",
      "   Train Batch [1100/1563]\n",
      "   Train Batch [1200/1563]\n",
      "   Train Batch [1300/1563]\n",
      "   Train Batch [1400/1563]\n",
      "   Train Batch [1500/1563]\n",
      "   Train Batch [1563/1563]\n",
      "Finished train phase for Epoch 8.\n",
      "train Loss: 0.3850 Acc: 0.8653\n",
      "Starting val phase for Epoch 8...\n",
      "   Val Batch [1/313]\n",
      "   Val Batch [100/313]\n",
      "   Val Batch [200/313]\n",
      "   Val Batch [300/313]\n",
      "   Val Batch [313/313]\n",
      "Finished val phase for Epoch 8.\n",
      "val Loss: 0.1427 Acc: 0.9510\n",
      "\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "Starting train phase for Epoch 9...\n",
      "   Train Batch [1/1563]\n",
      "   Train Batch [100/1563]\n",
      "   Train Batch [200/1563]\n",
      "   Train Batch [300/1563]\n",
      "   Train Batch [400/1563]\n",
      "   Train Batch [500/1563]\n",
      "   Train Batch [600/1563]\n",
      "   Train Batch [700/1563]\n",
      "   Train Batch [800/1563]\n",
      "   Train Batch [900/1563]\n",
      "   Train Batch [1000/1563]\n",
      "   Train Batch [1100/1563]\n",
      "   Train Batch [1200/1563]\n",
      "   Train Batch [1300/1563]\n",
      "   Train Batch [1400/1563]\n",
      "   Train Batch [1500/1563]\n",
      "   Train Batch [1563/1563]\n",
      "Finished train phase for Epoch 9.\n",
      "train Loss: 0.3669 Acc: 0.8736\n",
      "Starting val phase for Epoch 9...\n",
      "   Val Batch [1/313]\n",
      "   Val Batch [100/313]\n",
      "   Val Batch [200/313]\n",
      "   Val Batch [300/313]\n",
      "   Val Batch [313/313]\n",
      "Finished val phase for Epoch 9.\n",
      "val Loss: 0.1389 Acc: 0.9523\n",
      "\n",
      "Training complete in 158m 12s\n",
      "Best val Acc: 0.952300\n",
      "Model saved to app/image_classification_model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "\n",
    "#device agnostic code and trying to use my own gpu\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    # For MPS, num_workers > 0 might not provide benefit and can sometimes cause issues.\n",
    "    # It's often recommended to stick to 0 for MPS DataLoaders.\n",
    "    # If you experience hangs later, ensure this remains 0.\n",
    "    num_workers_dataloader = 0\n",
    "elif torch.cuda.is_available(): # For external NVIDIA GPUs (unlikely on M4 Air)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    num_workers_dataloader = 4 # Or adjust based on your external GPU setup\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    # For CPU, num_workers=0 is most robust for avoiding hangs.\n",
    "    # For a 10-core CPU, you could try 2-4 workers if you experience long data loading times\n",
    "    # AFTER the initial batches start moving, but stick to 0 for robustness.\n",
    "    num_workers_dataloader = 0\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"DataLoader num_workers set to: {num_workers_dataloader}\")\n",
    "\n",
    "# --- Step 3: Data Preparation (CIFAR-10) ---\n",
    "# Define CIFAR-10 specific normalization values (mean and std)\n",
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# Define transformations for training and validation sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(cifar10_mean, cifar10_std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(cifar10_mean, cifar10_std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load CIFAR-10 Dataset\n",
    "data_root_path = './data' # Define path to data directory\n",
    "os.makedirs(data_root_path, exist_ok=True) # Ensure data directory exists\n",
    "\n",
    "print(\"Loading train_dataset...\")\n",
    "train_dataset = datasets.CIFAR10(root=data_root_path, train=True, download=True, transform=data_transforms['train'])\n",
    "print(\"train_dataset loaded.\")\n",
    "\n",
    "print(\"Loading val_dataset...\")\n",
    "val_dataset = datasets.CIFAR10(root=data_root_path, train=False, download=True, transform=data_transforms['val'])\n",
    "print(\"val_dataset loaded.\")\n",
    "\n",
    "# Create a dictionary of datasets\n",
    "image_datasets = {'train': train_dataset, 'val': val_dataset}\n",
    "\n",
    "# Create DataLoaders\n",
    "print(\"Creating DataLoaders...\")\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32,\n",
    "                             shuffle=True if x == 'train' else False,\n",
    "                             num_workers=num_workers_dataloader,\n",
    "                             pin_memory=True if device.type == 'mps' else False # Pin memory for MPS/CUDA if available\n",
    "                            )\n",
    "               for x in ['train', 'val']}\n",
    "print(\"DataLoaders created.\")\n",
    "\n",
    "# Get dataset sizes and class names\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print(f\"Dataset sizes: {dataset_sizes}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# --- Step 4: Model Loading and Modification (timm) ---\n",
    "print(\"Creating model...\")\n",
    "model_name = 'resnet18'\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "# Modify the final classification layer\n",
    "if hasattr(model, 'fc'):\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "elif hasattr(model, 'classifier'):\n",
    "    num_ftrs = model.classifier.in_features\n",
    "    model.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "elif hasattr(model, 'head'):\n",
    "    if isinstance(model.head, nn.Sequential) and isinstance(model.head[-1], nn.Linear):\n",
    "        num_ftrs = model.head[-1].in_features\n",
    "        model.head[-1] = nn.Linear(num_ftrs, num_classes)\n",
    "    elif isinstance(model.head, nn.Linear):\n",
    "        num_ftrs = model.head.in_features\n",
    "        model.head = nn.Linear(num_ftrs, num_classes)\n",
    "else:\n",
    "    raise AttributeError(f\"Couldn't find a common classification head for model: {model_name}. Please inspect its structure or choose another model.\")\n",
    "print(\"Model created and modified.\")\n",
    "\n",
    "# Move the model to the chosen device (MPS or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# --- Step 5: Training Loop ---\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            print(f\"Starting {phase} phase for Epoch {epoch}...\")\n",
    "            total_batches = len(dataloaders[phase])\n",
    "\n",
    "            for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                # Print progress every 100 batches, or at the start/end of phase\n",
    "                if (i + 1) % 100 == 0 or (i + 1) == total_batches or (i + 1) == 1:\n",
    "                    print(f\"   {phase.capitalize()} Batch [{i+1}/{total_batches}]\")\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            print(f\"Finished {phase} phase for Epoch {epoch}.\")\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.float() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# This ensures the training function is called when the notebook is run directly\n",
    "if __name__ == '__main__':\n",
    "    model_trained = train_model(model, criterion, optimizer, scheduler, num_epochs=10)\n",
    "    # --- Step 6: Saving the Trained Model ---\n",
    "    # Save the state_dict (weights) of your best performing model.\n",
    "    model_save_path = 'app/image_classification_model.pth'\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True) # Ensure 'app' directory exists\n",
    "    torch.save(model_trained.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c7063-ef52-402e-a9ed-9827687513d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Image Classifier (venv)",
   "language": "python",
   "name": "ml_image_classifier_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
